{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [tokenizer - hugging face](https://huggingface.co/docs/transformers/main_classes/tokenizer)\n",
    "- [日本語トークナイザの違いは下流タスク性能に影響を与えるか](https://www.anlp.jp/proceedings/annual_meeting/2023/pdf_dir/Q6-1.pdf)\n",
    "  - 実験の結果、形態素解析器ありの方がなしよりも、全タスクの性能の平均値において 2.0 ポイント向上することが確認されたが、形態素解析器の違いや単語分かち書き辞書の違いによる性能の向上は見られなかった。\n",
    "  - また、サブワード分割手法にWordPiece を用いた場合、BPE および Unigram と比較して、性能が 1.0 ポイント低下することが確認された。\n",
    "  - 最後に、トークナイザ辞書の類似性が下流タスクの性能の差と相関があることを示した。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"吾輩は猫である。名前はまだない。\n",
    "\n",
    "どこで生れたか頓と見当がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。吾輩はここで始めて人間というものを見た。しかもあとで聞くとそれは書生という人間中で一番獰悪な種族であったそうだ。この書生というのは時々我々を捕えて煮て食うという話である。しかしその当時は何という考もなかったから別段恐しいとも思わなかった。ただ彼の掌に載せられてスーと持ち上げられた時何だかフワフワした感じがあったばかりである。掌の上で少し落ち付いて書生の顔を見たのがいわゆる人間というものの見始であろう。この時妙なものだと思った感じが今でも残っている。第一毛を以て装飾されべきはずの顔がつるつるしてまるで薬缶だ。その後猫にも大分逢ったがこんな片輪には一度も出会わした事がない。のみならず顔の真中が余りに突起している。そうしてその穴の中から時々ぷうぷうと烟を吹く。どうも咽せぽくて実に弱った。これが人間の飲む烟草というものである事は漸くこの頃知った。\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n",
      "['吾', '##輩', 'は', '猫', 'で', 'ある', '。', '名前', 'は', 'まだ', 'ない', '。', 'どこ', 'で', '生']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertJapaneseTokenizer\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-v3')\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(len(tokenized_text))\n",
    "print(tokenized_text[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "330\n",
      "['吾', '##輩', 'は', '猫', 'で', 'ある', '。', '名前', 'は', 'まだ', 'ない', '。', 'どこ', 'で', '生']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-large-japanese-v2')\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(len(tokenized_text))\n",
    "print(tokenized_text[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627\n",
      "['åĲ', '¾', 'è', '¼', '©', 'ãģ¯', 'ç', 'Į', '«', 'ãģ§', 'ãģĤ', 'ãĤĭ', 'ãĢĤ', 'åĲ', 'į', 'åī', 'į', 'ãģ¯', 'ãģ¾', 'ãģł', 'ãģª', 'ãģĦ', 'ãĢĤ', 'Ċ', 'Ċ', 'ãģ', '©', 'ãģĵ', 'ãģ§', 'çĶŁ', 'ãĤĮ', 'ãģŁ', 'ãģĭ', 'é', 'ł', 'ĵ', 'ãģ¨', 'è¦', 'ĭ', 'å½', 'ĵ', 'ãģĮ', 'ãģ', '¤', 'ãģĭ', 'ãģ', '¬', 'ãĢĤ', 'ä½', 'ķ', 'ãģ§', 'ãĤĤ', 'è', 'ĸ', 'Ħ', 'æ', 'ļ', 'Ĺ', 'ãģĦ', 'ãģ', 'ĺ', 'ãĤ', 'ģ', 'ãģ', 'ĺ', 'ãĤ', 'ģ', 'ãģĹ', 'ãģŁ', 'æī', 'Ģ', 'ãģ§', 'ãĥĭ', 'ãĥ£', 'ãĥ¼ãĥ', 'ĭ', 'ãĥ£', 'ãĥ¼', 'æ³', '£', 'ãģĦ', 'ãģ¦', 'ãģĦ', 'ãģŁ', 'äº', 'ĭ', 'ãģł', 'ãģ', 'ĳ', 'ãģ¯', 'è', '¨', 'ĺ', 'æ', 'Ĩ', '¶', 'ãģĹ', 'ãģ¦', 'ãģĦ', 'ãĤĭ', 'ãĢĤ', 'åĲ', '¾', 'è', '¼', '©', 'ãģ¯', 'ãģĵ', 'ãģĵ', 'ãģ§', 'å§', 'ĭ', 'ãĤ', 'ģ', 'ãģ¦', 'äºº', 'éĸ', 'ĵ', 'ãģ¨', 'ãģĦ', 'ãģĨ', 'ãĤĤ', 'ãģ®', 'ãĤĴ', 'è¦', 'ĭ', 'ãģŁ', 'ãĢĤ', 'ãģĹ', 'ãģĭ', 'ãĤĤ', 'ãģĤ', 'ãģ¨', 'ãģ§', 'è', 'ģ', 'ŀ', 'ãģı', 'ãģ¨', 'ãģ', 'Ŀ', 'ãĤĮ', 'ãģ¯', 'æ', 'Ľ', '¸', 'çĶŁ', 'ãģ¨', 'ãģĦ', 'ãģĨ', 'äºº', 'éĸ', 'ĵ', 'ä¸Ń', 'ãģ§', 'ä¸Ģ', 'çķ', 'ª', 'ç', 'į', '°', 'æ', 'Ĥª', 'ãģª', 'ç', '¨', '®', 'æĹ', 'ı', 'ãģ§', 'ãģĤ', 'ãģ£', 'ãģŁ', 'ãģ', 'Ŀ', 'ãģĨ', 'ãģł', 'ãĢĤ', 'ãģĵ', 'ãģ®æ', 'Ľ', '¸', 'çĶŁ', 'ãģ¨', 'ãģĦ', 'ãģĨ', 'ãģ®', 'ãģ¯', 'æ', 'ĻĤ', 'ãĢ', 'ħ', 'æĪ', 'ĳ', 'ãĢ', 'ħ', 'ãĤĴ', 'æ', 'į', 'ķ', 'ãģ', 'Ī', 'ãģ¦', 'ç', 'ħ', '®', 'ãģ¦', 'é£', 'Ł', 'ãģĨ', 'ãģ¨', 'ãģĦ', 'ãģĨ', 'è', '©', '±', 'ãģ§', 'ãģĤ', 'ãĤĭ', 'ãĢĤ', 'ãģĹ', 'ãģĭ', 'ãģĹ', 'ãģ', 'Ŀ', 'ãģ®å', '½', 'ĵ', 'æ', 'ĻĤ', 'ãģ¯', 'ä½', 'ķ', 'ãģ¨', 'ãģĦ', 'ãģĨ', 'èĢ', 'ĥ', 'ãĤĤ', 'ãģª', 'ãģĭ', 'ãģ£', 'ãģŁ', 'ãģĭ', 'ãĤī', 'åĪ', '¥', 'æ', '®', 'µ', 'æ', 'ģ', 'Ĳ', 'ãģĹ', 'ãģĦ', 'ãģ¨', 'ãĤĤ', 'æĢ', 'Ŀ', 'ãĤ', 'ı', 'ãģª', 'ãģĭ', 'ãģ£', 'ãģŁ', 'ãĢĤ', 'ãģŁ', 'ãģł', 'å½', '¼', 'ãģ®æ', 'İ', 'Į', 'ãģ«', 'è', '¼', 'ī', 'ãģ', 'Ľ', 'ãĤī', 'ãĤĮ', 'ãģ¦', 'ãĤ¹', 'ãĥ¼', 'ãģ¨', 'æ', 'Į', 'ģ', 'ãģ', '¡', 'ä¸Ĭ', 'ãģ', 'Ĵ', 'ãĤī', 'ãĤĮ', 'ãģŁ', 'æ', 'ĻĤ', 'ä½', 'ķ', 'ãģł', 'ãģĭ', 'ãĥķ', 'ãĥ¯', 'ãĥķ', 'ãĥ¯', 'ãģĹ', 'ãģŁ', 'æĦ', 'Ł', 'ãģ', 'ĺ', 'ãģĮ', 'ãģĤ', 'ãģ£', 'ãģŁ', 'ãģ', '°', 'ãģĭ', 'ãĤĬ', 'ãģ§', 'ãģĤ', 'ãĤĭ', 'ãĢĤ', 'æ', 'İ', 'Į', 'ãģ®', 'ä¸Ĭ', 'ãģ§', 'å°', 'ĳ', 'ãģĹ', 'è', 'Ĳ', '½', 'ãģ', '¡', 'ä»', 'ĺ', 'ãģĦ', 'ãģ¦', 'æ', 'Ľ', '¸', 'çĶŁ', 'ãģ®é', '¡', 'Ķ', 'ãĤĴ', 'è¦', 'ĭ', 'ãģŁ', 'ãģ®', 'ãģĮ', 'ãģĦ', 'ãĤ', 'ı', 'ãĤ', 'Ĩ', 'ãĤĭ', 'äºº', 'éĸ', 'ĵ', 'ãģ¨', 'ãģĦ', 'ãģĨ', 'ãĤĤ', 'ãģ®', 'ãģ®', 'è¦', 'ĭ', 'å§', 'ĭ', 'ãģ§', 'ãģĤ', 'ãĤ', 'į', 'ãģĨ', 'ãĢĤ', 'ãģĵ', 'ãģ®æ', 'ĻĤ', 'å¦', 'Ļ', 'ãģª', 'ãĤĤ', 'ãģ®', 'ãģł', 'ãģ¨', 'æĢ', 'Ŀ', 'ãģ£', 'ãģŁ', 'æĦ', 'Ł', 'ãģ', 'ĺ', 'ãģĮ', 'ä»', 'Ĭ', 'ãģ§', 'ãĤĤ', 'æ', '®', 'ĭ', 'ãģ£', 'ãģ¦', 'ãģĦ', 'ãĤĭ', 'ãĢĤ', 'ç', '¬', '¬', 'ä¸Ģ', 'æ', '¯', 'Ľ', 'ãĤĴ', 'ä»', '¥', 'ãģ¦', 'è£ħ', 'é£', '¾', 'ãģķ', 'ãĤĮ', 'ãģ', '¹', 'ãģį', 'ãģ¯', 'ãģ', 'ļ', 'ãģ®é', '¡', 'Ķ', 'ãģĮ', 'ãģ', '¤', 'ãĤĭ', 'ãģ', '¤', 'ãĤĭ', 'ãģĹ', 'ãģ¦', 'ãģ¾', 'ãĤĭ', 'ãģ§', 'è', 'ĸ', '¬', 'ç', '¼', '¶', 'ãģł', 'ãĢĤ', 'ãģ', 'Ŀ', 'ãģ®å', '¾', 'Į', 'ç', 'Į', '«', 'ãģ«', 'ãĤĤ', 'å¤§', 'åĪ', 'Ĩ', 'éĢ', '¢', 'ãģ£', 'ãģŁ', 'ãģĮ', 'ãģĵ', 'ãĤĵ', 'ãģª', 'çī', 'ĩ', 'è', '¼', 'ª', 'ãģ«', 'ãģ¯', 'ä¸Ģ', 'åº', '¦', 'ãĤĤ', 'åĩ', 'º', 'ä¼', 'ļ', 'ãĤ', 'ı', 'ãģĹ', 'ãģŁ', 'äº', 'ĭ', 'ãģĮ', 'ãģª', 'ãģĦ', 'ãĢĤ', 'ãģ®', 'ãģ', '¿', 'ãģª', 'ãĤī', 'ãģ', 'ļé', '¡', 'Ķ', 'ãģ®ç', 'ľ', 'Ł', 'ä¸Ń', 'ãģĮ', 'ä½', 'Ļ', 'ãĤĬ', 'ãģ«', 'ç', 'ª', 'ģ', 'è', 'µ', '·', 'ãģĹ', 'ãģ¦', 'ãģĦ', 'ãĤĭ', 'ãĢĤ', 'ãģ', 'Ŀ', 'ãģĨ', 'ãģĹ', 'ãģ¦', 'ãģ', 'Ŀ', 'ãģ®ç', '©', '´', 'ãģ®', 'ä¸Ń', 'ãģĭ', 'ãĤī', 'æ', 'ĻĤ', 'ãĢ', 'ħ', 'ãģ', '·', 'ãģĨ', 'ãģ', '·', 'ãģĨ', 'ãģ¨', 'ç', 'ĥ', 'Ł', 'ãĤĴ', 'åĲ', '¹', 'ãģı', 'ãĢĤ', 'ãģ', '©', 'ãģĨ', 'ãĤĤ', 'å', 'Ĵ', '½', 'ãģ', 'Ľ', 'ãģ', '½', 'ãģı', 'ãģ¦', 'å®', 'Ł', 'ãģ«', 'å¼', '±', 'ãģ£', 'ãģŁ', 'ãĢĤ', 'ãģĵ', 'ãĤĮ', 'ãģĮ', 'äºº', 'éĸ', 'ĵ', 'ãģ®é', '£', '²', 'ãĤ', 'Ģ', 'ç', 'ĥ', 'Ł', 'è', 'į', 'ī', 'ãģ¨', 'ãģĦ', 'ãģĨ', 'ãĤĤ', 'ãģ®', 'ãģ§', 'ãģĤ', 'ãĤĭ', 'äº', 'ĭ', 'ãģ¯', 'æ', '¼', '¸', 'ãģı', 'ãģĵ', 'ãģ®é', 'ł', 'ĥ', 'ç', 'Ł', '¥', 'ãģ£', 'ãģŁ', 'ãĢĤ']\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(len(tokenized_text))\n",
    "print(tokenized_text[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n",
      "['▁', '吾', '輩', 'は', '猫', 'である', '。', '名前', 'はまだ', 'ない', '。', '▁', 'どこで', '生', 'れた', 'か', '頓', 'と見', '当', 'がつか', 'ぬ', '。', '何でも', '薄', '暗い', 'じめ', 'じめ', 'した', '所で', 'ニャ', 'ー', 'ニャ', 'ー', '泣', 'いていた', '事', 'だけは', '記憶', 'している', '。', '吾', '輩', 'は', 'ここで', '始めて', '人間', 'というもの', 'を見た', '。', 'しかも', 'あとで', '聞くと', 'それは', '書', '生', 'という', '人間', '中で', '一番', '獰', '悪な', '種族', 'であった', 'そうだ', '。', 'この', '書', '生', 'というのは', '時々', '我々', 'を捕', 'えて', '煮', 'て', '食', 'う', 'という話', 'である', '。', 'しかし', 'その', '当時は', '何', 'という', '考', 'もなかった', 'から', '別', '段', '恐', 'しい', 'とも思', 'わなかった', '。', 'ただ', '彼の', '掌', 'に', '載せ', 'られて', 'スー', 'と', '持ち', '上げられた', '時', '何だか', 'フ', 'ワ', 'フ', 'ワ', 'した', '感じ', 'があった', 'ばかり', 'である', '。', '掌', 'の上で', '少し', '落ち', '付いて', '書', '生の', '顔', 'を見た', 'のが', 'いわゆる', '人間', 'という', 'ものの', '見', '始', 'であろう', '。', 'この時', '妙', 'なもの', 'だと思った', '感じが', '今でも', '残っている', '。', '第一', '毛', 'を', '以て', '装飾', 'され', 'べき', 'はずの', '顔', 'がつ', 'る', 'つる', 'して', 'まるで', '薬', '缶', 'だ', '。', 'その後', '猫', 'にも', '大分', '逢', 'ったが', 'こんな', '片', '輪', 'には', '一度も', '出会', 'わ', 'した事', 'がない', '。', 'のみならず', '顔', 'の真', '中', 'が', '余', 'りに', '突起', 'している', '。', 'そう', 'してその', '穴', 'の中から', '時々', 'ぷ', 'う', 'ぷ', 'うと', '烟', 'を', '吹く', '。', 'どうも', '咽', 'せ', 'ぽ', 'くて', '実に', '弱', 'った', '。', 'これが', '人間の', '飲む', '烟', '草', 'という', 'ものである', '事は', '漸', 'く', 'この頃', '知った', '。']\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt-1b\")\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(len(tokenized_text))\n",
    "print(tokenized_text[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245\n",
      "['▁', '吾', '輩', 'は', '猫', 'である', '。', '名前', 'はまだ', 'ない', '。', '▁', 'どこ', 'で', '生', 'れた', 'か', '頓', 'と', '見', '当', 'が', 'つか', 'ぬ', '。', '何', 'でも', '薄', '暗', 'いじめ', 'じ', 'め', 'した', '所で', 'ニャー', 'ニャー', '泣', 'いていた', '事', 'だけは', '記憶', 'している', '。', '吾', '輩', 'は', 'ここで', '始め', 'て', '人間', 'というもの', 'を見た', '。', 'しかも', 'あと', 'で', '聞く', 'と', 'それは', '書', '生', 'という', '人間', '中で', '一番', '獰', '悪', 'な', '種族', 'であった', 'そう', 'だ', '。', 'この', '書', '生', 'というのは', '時々', '我々', 'を', '捕', 'えて', '煮', 'て', '食', 'う', 'という話', 'である', '。', 'しかし', 'その', '当時は', '何', 'という', '考', 'も', 'なかった', 'から', '別', '段', '恐', 'しい', 'とも', '思', 'わなかった', '。', 'ただ', '彼の', '掌', 'に', '載せ', 'られて', 'スー', 'と', '持ち上げ', 'られた', '時', '何', 'だ', 'か', 'フ', 'ワ', 'フ', 'ワ', 'した', '感じ', 'があった', 'ばかり', 'である', '。', '掌', 'の上で', '少し', '落ち', '付いて', '書', '生', 'の顔', 'を見た', 'の', 'が', 'いわゆる', '人間', 'というもの', 'の', '見', '始', 'であろう', '。', 'この時', '妙', 'な', 'ものだ', 'と思った', '感じ', 'が', '今でも', '残っている', '。', '第一', '毛', 'を以て', '装飾', 'され', 'べき', 'はずの', '顔', 'が', 'つる', 'つる', 'して', 'まるで', '薬', '缶', 'だ', '。', 'その後', '猫', 'にも', '大分', '逢', 'ったが', 'こんな', '片', '輪', 'には', '一度も', '出', '会', 'わ', 'した', '事', 'がない', '。', 'のみならず', '顔', 'の', '真', '中', 'が', '余り', 'に', '突起', 'している', '。', 'そう', 'して', 'その', '穴', 'の中から', '時々', 'ぷ', 'う', 'ぷ', 'う', 'と', '烟', 'を', '吹く', '。', 'どう', 'も', '咽', 'せ', 'ぽ', 'く', 'て', '実に', '弱', 'った', '。', 'これが', '人間の', '飲む', '烟', '草', 'というものである', '事', 'は', '漸', 'く', 'この頃', '知', 'った', '。']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-small\")\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(len(tokenized_text))\n",
    "print(tokenized_text[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "print(len(tokenized_text))\n",
    "print(tokenized_text[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
